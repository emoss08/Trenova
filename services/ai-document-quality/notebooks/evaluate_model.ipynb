{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Quality Assessment - Model Evaluation\n",
    "\n",
    "This notebook provides interactive evaluation of the document quality assessment model.\n",
    "\n",
    "## Features:\n",
    "- Load trained model and test data\n",
    "- Calculate comprehensive metrics\n",
    "- Generate visualizations\n",
    "- Test on individual images with explanations\n",
    "- Analyze errors and edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import project modules\n",
    "from src.data.dataset import DocumentDataset\n",
    "from src.models.model import DocumentQualityModel, ModelConfig\n",
    "from src.evaluation.metrics import evaluate_model_comprehensive\n",
    "from src.evaluation.visualize import (\n",
    "    plot_calibration_curve,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curves,\n",
    "    plot_prediction_distribution,\n",
    "    plot_issue_analysis,\n",
    ")\n",
    "from src.evaluation.explainability import visualize_explanation, get_target_layer\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = project_root / \"models\" / \"best_model.pth\"\n",
    "CONFIG_PATH = project_root / \"config\" / \"best_training.yaml\"\n",
    "\n",
    "# Load config\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model config\n",
    "model_config = ModelConfig(\n",
    "    backbone=config.get(\"model\", {}).get(\"backbone\", \"efficientnet_b0\"),\n",
    "    num_quality_classes=config.get(\"model\", {}).get(\"num_quality_classes\", 5),\n",
    "    num_issue_classes=config.get(\"model\", {}).get(\"num_issue_classes\", 10),\n",
    "    hidden_dim=config.get(\"model\", {}).get(\"hidden_dim\", 256),\n",
    "    dropout_rate=config.get(\"model\", {}).get(\"dropout_rate\", 0.5),\n",
    "    use_attention=config.get(\"model\", {}).get(\"use_attention\", True),\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = DocumentQualityModel(model_config)\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Model loaded from {MODEL_PATH}\")\n",
    "print(f\"  Backbone: {model_config.backbone}\")\n",
    "print(f\"  Quality classes: {model_config.num_quality_classes}\")\n",
    "print(f\"  Issue classes: {model_config.num_issue_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find test metadata\n",
    "datasets_dir = project_root / \"datasets\"\n",
    "test_metadata = datasets_dir / \"default\" / \"test\" / \"test_metadata.csv\"\n",
    "\n",
    "if not test_metadata.exists():\n",
    "    print(f\"‚ùå Test metadata not found at {test_metadata}\")\n",
    "else:\n",
    "    # Create dataset\n",
    "    test_dataset = DocumentDataset(\n",
    "        metadata_file=str(test_metadata),\n",
    "        transform=\"test\",\n",
    "        use_advanced_augmentations=False,\n",
    "    )\n",
    "\n",
    "    # Create dataloader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(f\"‚úì Test dataset loaded: {len(test_dataset)} images\")\n",
    "    print(f\"  Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Running evaluation... (this may take a few minutes)\")\n",
    "evaluation_results = evaluate_model_comprehensive(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    acceptance_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Regression metrics\n",
    "reg = evaluation_results[\"regression\"]\n",
    "print(\"\\nüìä Regression Metrics (Quality Scores):\")\n",
    "print(f\"  MAE:  {reg['mae']:.4f}\")\n",
    "print(f\"  RMSE: {reg['rmse']:.4f}\")\n",
    "print(f\"  R¬≤:   {reg['r2']:.4f}\")\n",
    "print(f\"  Within 10%: {reg['within_10pct']:.2%}\")\n",
    "\n",
    "# Classification metrics\n",
    "cls = evaluation_results[\"classification\"]\n",
    "print(\"\\nüéØ Classification Metrics (Quality Classes):\")\n",
    "print(f\"  Accuracy:          {cls['accuracy']:.4f}\")\n",
    "print(f\"  Balanced Accuracy: {cls['balanced_accuracy']:.4f}\")\n",
    "print(f\"  Weighted F1:       {cls['weighted_f1']:.4f}\")\n",
    "\n",
    "# Binary classification\n",
    "binary = evaluation_results[\"binary_classification\"]\n",
    "print(\"\\n‚úÖ/‚ùå Binary Classification (Accept/Reject):\")\n",
    "print(f\"  F1 Score: {binary['f1']:.4f}\")\n",
    "print(f\"  Precision: {binary['precision']:.4f}\")\n",
    "print(f\"  Recall: {binary['recall']:.4f}\")\n",
    "print(f\"  ROC AUC: {binary['roc_auc']:.4f}\")\n",
    "print(f\"  False Reject Rate: {binary['fpr']:.4f}\")\n",
    "\n",
    "# Calibration\n",
    "cal = evaluation_results[\"calibration\"]\n",
    "print(\"\\nüéöÔ∏è  Calibration:\")\n",
    "print(f\"  ECE (Expected Calibration Error): {cal['ece']:.4f}\")\n",
    "print(f\"  {'‚úì Well calibrated' if cal['ece'] < 0.1 else '‚ö† Needs calibration'}\")\n",
    "\n",
    "# Issue detection\n",
    "issues = evaluation_results[\"issue_detection\"]\n",
    "print(\"\\nüîç Issue Detection:\")\n",
    "print(f\"  Macro F1: {issues['macro_f1']:.4f}\")\n",
    "print(f\"  Micro F1: {issues['micro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(evaluation_results[\"calibration\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.array(evaluation_results[\"classification\"][\"confusion_matrix\"])\n",
    "plot_confusion_matrix(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_scores = np.array(evaluation_results[\"raw_predictions\"][\"quality_scores\"])\n",
    "quality_targets = np.array(evaluation_results[\"raw_predictions\"][\"quality_targets\"])\n",
    "binary_targets = (quality_targets >= 0.5).astype(int)\n",
    "\n",
    "plot_roc_curves(quality_scores, binary_targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_distribution(quality_scores, quality_targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Issue Detection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_issue_analysis(evaluation_results[\"issue_detection\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test on Individual Images with Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target layer for Grad-CAM\n",
    "target_layer = get_target_layer(model, model_config.backbone)\n",
    "\n",
    "# Test on a few sample images\n",
    "num_samples = 5\n",
    "sample_indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    # Load image\n",
    "    img_path = Path(test_dataset.image_paths[idx])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    # Generate explanation\n",
    "    predictions, vis_image = visualize_explanation(\n",
    "        image=image,\n",
    "        model=model,\n",
    "        transform=test_dataset.transform,\n",
    "        target_layer=target_layer,\n",
    "        save_path=None,\n",
    "    )\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Sample {idx + 1}: {img_path.name}\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find samples with largest errors\n",
    "errors = np.abs(quality_scores - quality_targets)\n",
    "worst_indices = np.argsort(errors)[-5:][::-1]\n",
    "\n",
    "print(\"üìâ Top 5 Worst Predictions:\\n\")\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    print(f\"{rank}. Index {idx}:\")\n",
    "    print(f\"   Predicted: {quality_scores[idx]:.3f}\")\n",
    "    print(f\"   Actual:    {quality_targets[idx]:.3f}\")\n",
    "    print(f\"   Error:     {errors[idx]:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize worst predictions\n",
    "print(\"\\nVisualizing worst predictions...\\n\")\n",
    "for idx in worst_indices[:3]:  # Show top 3\n",
    "    img_path = Path(test_dataset.image_paths[idx])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    predictions, vis_image = visualize_explanation(\n",
    "        image=image,\n",
    "        model=model,\n",
    "        transform=test_dataset.transform,\n",
    "        target_layer=target_layer,\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(vis_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\n",
    "        f\"Error: {errors[idx]:.3f} | Pred: {quality_scores[idx]:.3f} | True: {quality_targets[idx]:.3f}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Per-Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by quality class\n",
    "class_names = [\"High\", \"Good\", \"Moderate\", \"Poor\", \"Very Poor\"]\n",
    "quality_class_targets = np.array(evaluation_results[\"raw_predictions\"][\"quality_class_targets\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    # Get predictions for this class\n",
    "    mask = quality_class_targets == class_idx\n",
    "    class_scores = quality_scores[mask]\n",
    "    class_targets = quality_targets[mask]\n",
    "    \n",
    "    if len(class_scores) > 0:\n",
    "        ax = axes[class_idx]\n",
    "        ax.scatter(class_targets, class_scores, alpha=0.5, s=20)\n",
    "        ax.plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_xlabel(\"True Score\")\n",
    "        ax.set_ylabel(\"Predicted Score\")\n",
    "        ax.set_title(f\"{class_name}\\n(n={len(class_scores)})\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate MAE for this class\n",
    "        class_mae = np.mean(np.abs(class_scores - class_targets))\n",
    "        ax.text(0.05, 0.95, f\"MAE: {class_mae:.3f}\", \n",
    "               transform=ax.transAxes, va=\"top\", fontsize=10,\n",
    "               bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Prediction Performance by Quality Class\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\nüéØ Overall Performance:\")\n",
    "if reg[\"r2\"] > 0.8 and cls[\"balanced_accuracy\"] > 0.8 and cal[\"ece\"] < 0.1:\n",
    "    print(\"  ‚úÖ EXCELLENT - Model is performing well across all metrics\")\n",
    "elif reg[\"r2\"] > 0.6 and cls[\"balanced_accuracy\"] > 0.7:\n",
    "    print(\"  ‚ö†Ô∏è  GOOD - Model is usable but has room for improvement\")\n",
    "else:\n",
    "    print(\"  ‚ùå NEEDS IMPROVEMENT - Model requires retraining\")\n",
    "\n",
    "# Specific recommendations\n",
    "print(\"\\nüìã Recommendations:\")\n",
    "\n",
    "if cal[\"ece\"] > 0.1:\n",
    "    print(\"  ‚Ä¢ Model is poorly calibrated - consider temperature scaling\")\n",
    "\n",
    "if binary[\"fpr\"] > 0.1:\n",
    "    print(f\"  ‚Ä¢ High false reject rate ({binary['fpr']:.2%}) - may frustrate users\")\n",
    "\n",
    "if binary[\"fnr\"] > 0.1:\n",
    "    print(f\"  ‚Ä¢ High false accept rate ({binary['fnr']:.2%}) - quality control issue\")\n",
    "\n",
    "if cls[\"balanced_accuracy\"] < 0.7:\n",
    "    print(\"  ‚Ä¢ Poor class balance - use weighted sampling or class weights\")\n",
    "\n",
    "if reg[\"within_10pct\"] < 0.7:\n",
    "    print(\"  ‚Ä¢ Many predictions are off by >10% - model needs more training\")\n",
    "\n",
    "# Check if any class has very poor performance\n",
    "for class_name, metrics in cls[\"per_class\"].items():\n",
    "    if metrics[\"f1\"] < 0.5:\n",
    "        print(f\"  ‚Ä¢ '{class_name}' class has F1={metrics['f1']:.3f} - needs more training data\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"  1. Review misclassified examples (Section 6)\")\n",
    "print(\"  2. Analyze per-class performance (Section 7)\")\n",
    "print(\"  3. Collect more real-world training data if needed\")\n",
    "print(\"  4. Consider model calibration techniques\")\n",
    "print(\"  5. Test on production data before deployment\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save evaluation results for later analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "results_dir = project_root / \"evaluation_results\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save metrics as JSON\n",
    "serializable_results = {\n",
    "    k: v for k, v in evaluation_results.items() if k != \"raw_predictions\"\n",
    "}\n",
    "\n",
    "with open(results_dir / \"evaluation_metrics.json\", \"w\") as f:\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
